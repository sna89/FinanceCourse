{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting to work on the data, we need to obtain the data from the data sources. In order to do so I used web scraping technique and pandas read_csv function to collect the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping - daily_treasury_yield_curve_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://data.treasury.gov/feed.svc/DailyTreasuryYieldCurveRateData'\n",
    "COLUMNS = ['Date','1 Mo','2 Mo','3 Mo','6 Mo','1 Yr','2 Yr','3 Yr','5 Yr','7 Yr','10 Yr','20 Yr','30 Yr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(soup):\n",
    "    data = soup.find_all('m:properties')[0]\n",
    "    data = data.text\n",
    "    data = data.split('\\n')\n",
    "    data = data[2:-2]\n",
    "    return data\n",
    "\n",
    "def create_new_row(data,columns):\n",
    "    new_row = dict()\n",
    "    \n",
    "    for idx,(key,value) in enumerate(zip(columns,data)):\n",
    "        if idx == 0:\n",
    "            value = datetime.strptime(value, '%Y-%m-%dT%H:%M:%S')\n",
    "        else:\n",
    "            value = round(float(value),2) if value != '' else 0\n",
    "                \n",
    "        new_row[key] = value\n",
    "        \n",
    "    return new_row\n",
    "\n",
    "def get_data_from_link(link,columns):\n",
    "    sauce = urllib.request.urlopen(link).read()\n",
    "    soup = bs(sauce,'xml')\n",
    "    data = process_data(soup)\n",
    "    new_row = create_new_row(data,columns)\n",
    "    return new_row\n",
    "\n",
    "sauce = urllib.request.urlopen(URL).read()\n",
    "soup = bs(sauce,'xml')\n",
    "df = pd.DataFrame(columns=COLUMNS)\n",
    "for idx,link in enumerate(soup.find_all('id')):\n",
    "    if idx>0:\n",
    "        daily_yield_curve_rate_data = get_data_from_link(link.text,COLUMNS)\n",
    "        df = df.append(daily_yield_curve_rate_data,ignore_index=True)\n",
    "\n",
    "df.to_csv('daily_treasury_yield_curve_rate.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping - BAA10Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import requests\n",
    "\n",
    "URL = r'https://fred.stlouisfed.org/graph/fredgraph.csv?bgcolor=%23e1e9f0&chart_type=line&drp=0&fo=open%20sans&graph_bgcolor=%23ffffff&height=450&mode=fred&recession_bars=on&txtcolor=%23444444&ts=12&tts=12&width=1168&nt=0&thu=0&trc=0&show_legend=yes&show_axis_titles=yes&show_tooltip=yes&id=BAA10Y&scale=left&cosd=1986-01-02&coed=2019-12-03&line_color=%234572a7&link_values=false&line_style=solid&mark_type=none&mw=3&lw=2&ost=-99999&oet=99999&mma=0&fml=a&fq=Daily&fam=avg&fgst=lin&fgsnd=2009-06-01&line_index=1&transformation=lin&vintage_date=2019-12-05&revision_date=2019-12-05&nd=1990-02-02'\n",
    "c = pd.read_csv(URL)\n",
    "print(c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
