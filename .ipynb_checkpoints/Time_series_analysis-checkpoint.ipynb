{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "daily_treasury_yield_curve_rate_df = pd.read_csv('daily_treasury_yield_curve_rate.csv',\n",
    "                                                 parse_dates=['Date'], index_col='Date')\n",
    "baa10y_df = pd.read_csv('BAA10Y.csv', parse_dates=['DATE'], index_col='DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIODS = ['3 Mo','1 Yr','5 Yr','10 Yr','30 Yr']\n",
    "FONT_SIZE = 10\n",
    "FIGSIZE = (10,5)\n",
    "\n",
    "TRAIN_START_DATE_1 ='1990-01-01'\n",
    "TRAIN_END_DATE_1 = '2006-01-01'\n",
    "TEST_START_DATE_1 = '2006-01-01'\n",
    "TEST_END_DATE_1 = '2007-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.a Finding trend and seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can decompose time series to 3 main components: trend, seasonality, and noise (random vairable).\n",
    "There are 2 main models that uses these components:\n",
    "\n",
    "1. Additive model: y = trend + seasonality + noise\n",
    "2. Multiplicative model: y = trend x seasonality x noise\n",
    "\n",
    "In multiplicative model the variance of the seasonality is increasing (or decreasing) over time, while in additive model the varaince of the seasonality doesn't change much.\n",
    "I used seasonal_decompose function (from statsmodels package) to decompose the time series into trend and seasonality. This function requires:\n",
    "\n",
    "1. Model type. \n",
    "2. Frequency (which can be thought of as the seasonality)\n",
    "3. No missing data. \n",
    "\n",
    "Due to this, I checked the rolling standart deviation of each time series to decide the model type. Furthermore, for seasonality  period of time of 1 quarter (3 months) is used. (The seasonality changes for different periods of time).\n",
    "\n",
    "In this section, we were asked to work on data between 1990 to 2005."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PERIODS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3803db49ef42>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_data_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'std'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRAIN_START_DATE_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRAIN_END_DATE_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseries\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mseries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrolling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Noam\\Technion\\Second Degree\\Third semester\\Finance\\FinanceCourse\\Utils.ipynb\u001b[0m in \u001b[0;36mcreate_data_df\u001b[1;34m(use, start_date, end_date)\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PERIODS' is not defined"
     ]
    }
   ],
   "source": [
    "df_std = pd.DataFrame()\n",
    "data_df = create_data_df(use='std', start_date=TRAIN_START_DATE_1, end_date=TRAIN_END_DATE_1) \n",
    "\n",
    "for name, series in data_df.iteritems():\n",
    "    data = series.rolling(window=90).std()\n",
    "    df_std = add_data_to_df(df_std, name, data)\n",
    "\n",
    "plot_data(df_std, 'STD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the standart deviation is not increasing or decresing over time, but volatile, *I chose to use the additive model*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decompose(TRAIN_START_DATE_1, TRAIN_END_DATE_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the trend is a smoothed average of the no risk interest.\n",
    "The values of trend are between 1 to 8 while the values of seasonality are between -0.1 to 0.15, \n",
    "meaning the seasonality barely has any effect. It can be thought of that there is not seasonality for this period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, it can be seen that the interest rate of 30 years interest is constant 0 from ~2002. \n",
    "There is not any data for this interest between 2.2002 to 2.2006.\n",
    "We need to take it into account when forecasting since in VAR model predictions of all time series are affected by the all other\n",
    "time series. Meaning that the fact that the interest rate for 30 years is constant 0 for long time (at transformed data) will\n",
    "affect the forecast of, for example, 3 month interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.b Checking stationarity and unit root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The properties (definition) of (weak) stationarity time series is:\n",
    "\n",
    "1. Constant mean: for all t: E[y(t)] = M\n",
    "2. Auto covariance depends only on the diffrenece in time: for all u,v,k cov(y(u),y(v)) = cov(y(u+a),y(v+a))\n",
    "    \n",
    "It can be inferred from the definition that in stationary time series there isn''t trend or seasonality. \n",
    "In all periods there is a trend, so we can assume that all serieses are not stationary.\n",
    "\n",
    "For formality, I used Augmented Dickey-Fuller test, which is a statisitcal method which checks if time series is stationary:\n",
    "1. The null hypothesis (H0) suggests that the is a unit root, meaning the series is not stationary . \n",
    "The alternative hypothesis (H1) suggests that there is not an unit root and that the series is stationary.\n",
    "2. We fail to reject H0 if p-value > 0.05. (non stationary). \n",
    "We reject H0 if p-value <= 0.05 (statonary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import adfuller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_data_df(use='adfuller', start_date=TRAIN_START_DATE_1, end_date=TRAIN_END_DATE_1) \n",
    "train_df = train_df.drop(columns=['30 Yr'])\n",
    "train_df = train_df.dropna()\n",
    "\n",
    "test_df = create_data_df(use='dataset', start_date=TEST_START_DATE_1, end_date=TEST_END_DATE_1)\n",
    "test_df = test_df.drop(columns=['30 Yr'])\n",
    "test_df = test_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.c Co-Integration test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the Co-integration test we need to transform each time series to a stationary time series.\n",
    "It can be done by trnasforming the time series to the 1 lag difference time series: \n",
    "y_t = x_t - x_t-1 (where x is the original time series and x_t is the value of the series at time t)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_train_df = train_df.diff(periods=1).dropna()\n",
    "test_stationarity(diff_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_train_df = diff_train_df\n",
    "plot_data(stationary_train_df, 'transformed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is stationary we can check if co-integration exists using johansen test.\n",
    "I applied the test for 10 lags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.vector_ar.vecm import select_coint_rank\n",
    "LAGS = 11\n",
    "\n",
    "# stationary_train_df = stationary_train_df.drop(columns=['30 Yr'])\n",
    "for lag in range(1,LAGS):\n",
    "    print(\"lag = {lag}\".format(lag=lag))\n",
    "    print('#'*50)\n",
    "    res = select_coint_rank(stationary_train_df, 1, lag, method='trace')\n",
    "    print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that for each lag in the specified lag range the johansen output the hishest possible rank:\n",
    "For each rank the test statistic is higher than the critical value.\n",
    "Now we can use a VAR (Vector auto regressive) model to predict future values.\n",
    "For each lag we are constructing different model and we need to chose most appropriate model according to some criteria.\n",
    "I chose AIC criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import statsmodels.tsa.api as smt\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "\n",
    "mod = smt.VAR(stationary_train_df)\n",
    "lag_results = mod.select_order(maxlags=LAGS)\n",
    "chosen_lag = lag_results.selected_orders['aic']\n",
    "fitted_model = mod.fit(maxlags=chosen_lag, trend='ct')\n",
    "# print(lag_results.summary())\n",
    "print(fitted_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Durbinâ€“Watson statistic in order to detect if there is auto correlation at lag 1 of the prediction errors (residuals)\n",
    "from regression analysis. The null hypothesis states that the residuals are uncorrelated and the alternative hypothesis stated that\n",
    "the residuals follow first order auto regressive process. \n",
    "The value of the test statistic, d, is between 0 to 4.\n",
    "d < 2: postive serial correlation\n",
    "d > 2: negative serial correlation\n",
    "d=2: no auto correlation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(fitted_model.resid)\n",
    "\n",
    "for col, val in zip(stationary_train_df.columns, out):\n",
    "    print((col), ':', round(val,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the fitted model in order to forecast future values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "forecast = fitted_model.forecast(stationary_train_df.values[-chosen_lag:],test_df.shape[0])\n",
    "forecast_df = pd.DataFrame(data=forecast, columns=stationary_train_df.columns)\n",
    "forecast_df.index = test_df.index\n",
    "plot_data(forecast_df, 'stationary forecast')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our prediction we need to transform the data to its original form. In previous stage we transformed the data to stationary data using 1 period difference series. (y_t = x_t - x_t-1)\n",
    "We need to obtain x_t from y_t: \n",
    "\n",
    "x_t = y_t + x_t-1 and x_t-1 = y_t-1 + x_t-2 \n",
    "\n",
    "x_t = y_t + y_t-1 + x_t-2 and x_t-2 = y_t-2 + x_t-3 \n",
    "\n",
    "x_t = y_t + y_t-1 + y_t-2 + x_t-3 \n",
    "...\n",
    "\n",
    "In order to get x_t we need calculate the cumulative sum of all previous samples from the stationary data plus the first sample from the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_df.iloc[-1]\n",
    "plot_prediction(forecast_df, test_df, train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen from the forcast results that after several steps the forecast is constant \n",
    "and does not give any additional information.\n",
    "We can use this forecast or try predict in iterative way, when each time we predict only the next lag (3) observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_test_df = test_df.diff(periods=1).dropna()\n",
    "stationary_test_df = diff_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterative_forecast_df = pd.DataFrame()\n",
    "date_series = pd.Series()\n",
    "nobs = stationary_test_df.shape[0]\n",
    "for idx in range(0,nobs,chosen_lag):\n",
    "    s_idx = idx\n",
    "    f_idx = min(nobs, idx + chosen_lag)\n",
    "    inner_stationary_test_df = stationary_test_df.iloc[s_idx: f_idx]\n",
    "    if inner_stationary_test_df.shape[0] == chosen_lag:\n",
    "        date = inner_stationary_test_df.index.values\n",
    "        date = pd.Series(date)\n",
    "        forecast = fitted_model.forecast(inner_stationary_test_df.values,chosen_lag)\n",
    "        inner_forecast_df = pd.DataFrame(data=forecast, columns=stationary_test_df.columns)\n",
    "        iterative_forecast_df = pd.concat([iterative_forecast_df, inner_forecast_df], axis=0)\n",
    "        date_series = pd.concat([date_series, date],axis=0)\n",
    "iterative_forecast_df.index = date_series.values   \n",
    "plot_data(iterative_forecast_df, 'iterative stationary forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction(iterative_forecast_df, test_df, train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('VAR Iterative forecast evaluation for 2006')\n",
    "print('-'*100)\n",
    "evaluate(iterative_forecast_df, test_df.iloc[:iterative_forecast_df.shape[0]])\n",
    "print('\\nVAR forecast evaluation for 2006')\n",
    "print('-'*100)\n",
    "evaluate(forecast_df, test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not use VAR to predict all time series together since there is missing data in 30 years interest series.\n",
    "In the next section I will complete the forecast for this time series, trying to predict the interest in 2006 at times when data is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_data_df(use='adfuller', start_date=TRAIN_START_DATE_1, end_date=TRAIN_END_DATE_1) \n",
    "train_df = train_df.dropna()\n",
    "\n",
    "test_df = create_data_df(use='dataset', start_date=TEST_START_DATE_1, end_date=TEST_END_DATE_1)\n",
    "test_df = test_df.dropna()\n",
    "\n",
    "stationary_train_df = train_df.diff(periods=1).dropna()\n",
    "stationary_test_df = test_df.diff(periods=1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smt.VAR(stationary_train_df)\n",
    "lag_results = mod.select_order(maxlags=LAGS)\n",
    "chosen_lag = lag_results.selected_orders['aic']\n",
    "fitted_model = mod.fit(maxlags=chosen_lag, trend='ct')\n",
    "forecast = fitted_model.forecast(stationary_train_df.values[-chosen_lag:], test_df.shape[0])\n",
    "forecast_df = pd.DataFrame(data=forecast, columns=stationary_train_df.columns)\n",
    "forecast_df.index = test_df.index\n",
    "train_sample = train_df.iloc[-1]\n",
    "plot_prediction(forecast_df, test_df, train_sample,['30 Yr'])\n",
    "evaluate(forecast_df, test_df, ['30 Yr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_DATE_2 ='2000-01-01'\n",
    "TRAIN_END_DATE_2 = '2016-01-01'\n",
    "TEST_START_DATE_2 = '2016-01-01'\n",
    "TEST_END_DATE_2 = '2017-01-01'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose(TRAIN_START_DATE_2, TRAIN_END_DATE_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = create_data_df(use='adfuller', start_date=TRAIN_START_DATE_2, end_date=TRAIN_END_DATE_2).dropna()\n",
    "test_df = create_data_df(use='dataset', start_date=TEST_START_DATE_2, end_date=TEST_END_DATE_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stationarity(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_train_df = train_df.diff(periods=1).dropna()\n",
    "test_stationarity(diff_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationary_train_df = diff_train_df\n",
    "plot_data(stationary_train_df, 'transformed_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Co - integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = select_coint_rank(stationary_train_df, 1, LAGS, method='trace')\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = smt.VAR(stationary_train_df)\n",
    "lag_results = mod.select_order(maxlags=LAGS)\n",
    "chosen_lag = lag_results.selected_orders['aic']\n",
    "fitted_model = mod.fit(maxlags=chosen_lag, trend='ct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "out = durbin_watson(fitted_model.resid)\n",
    "\n",
    "for col, val in zip(stationary_train_df.columns, out):\n",
    "    print((col), ':', round(val,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.e.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = fitted_model.forecast(stationary_train_df.values[-chosen_lag:],test_df.shape[0])\n",
    "forecast_df = pd.DataFrame(data=forecast, columns=stationary_train_df.columns)\n",
    "forecast_df.index = test_df.index\n",
    "plot_data(forecast_df, 'stationary forecast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_df.iloc[-1]\n",
    "plot_prediction(forecast_df, test_df, train_sample)\n",
    "print('VAR forecast evaluation for 2016')\n",
    "print('-'*100)\n",
    "evaluate(forecast_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
