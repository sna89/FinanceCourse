{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Utils.ipynb\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import import_ipynb\n",
    "from Utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def save_checkpoint(model,optimizer,filepath):\n",
    "    state = {\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict()\n",
    "    }\n",
    "    torch.save(state, filepath)\n",
    "\n",
    "def load_checkpoint(model,optimizer,filepath):\n",
    "    # \"lambda\" allows to load the model on cpu in case it is saved on gpu\n",
    "    state = torch.load(filepath,lambda storage, loc: storage)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    return model,optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_START_PERIOD = '1990-01-01'\n",
    "TRAIN_END_PERIOD = '2000-01-01'\n",
    "\n",
    "TEST_START_PERIOD = '2000-01-01'\n",
    "TEST_END_PERIOD = '2001-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FinanceDataset(Dataset):\n",
    "    def __init__(self, start_date, end_date, asset_to_forecast, time_to_forecast):\n",
    "        self.finance_data_df = create_data_df(use='dataset', start_date=start_date, end_date=end_date) \n",
    "        self.data_tensor = torch.from_numpy(self.finance_data_df.values)\n",
    "        \n",
    "        self.asset_to_forecast = asset_to_forecast\n",
    "        self.time_to_forecast = time_to_forecast\n",
    "        self.prepare_labels()\n",
    "        \n",
    "    def prepare_labels(self):\n",
    "        self.labels_df = self.finance_data_df.shift(-self.time_to_forecast)[self.asset_to_forecast]\n",
    "        self.labels_tensor = torch.from_numpy(self.labels_df.values)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.finance_data_df.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_tensor[idx], self.labels_tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, data_dim, output_size, hidden_dim, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim=hidden_dim\n",
    "\n",
    "        # define an RNN with specified parameters\n",
    "        # batch_first means that the first dim of the input and output will be the batch_size\n",
    "        self.rnn = nn.RNN(data_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        \n",
    "        # last, fully-connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        # x (batch_size, seq_length, input_size)\n",
    "        # hidden (n_layers, batch_size, hidden_dim)\n",
    "        # r_out (batch_size, time_step, hidden_size)\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # get RNN outputs\n",
    "        r_out, hidden = self.rnn(x, hidden)\n",
    "        # shape output to be (batch_size*seq_length, hidden_dim)\n",
    "        r_out = r_out.view(-1, self.hidden_dim)  \n",
    "        \n",
    "        # get final output \n",
    "        output = self.fc(r_out)\n",
    "        \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_finance_dataset = FinanceDataset(TRAIN_START_PERIOD, TRAIN_END_PERIOD, '3 Mo', 1)\n",
    "test_finance_dataset = FinanceDataset(TEST_START_PERIOD, TEST_END_PERIOD, '3 Mo', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(train_finance_dataset, batch_size=24, shuffle=False, num_workers=0)\n",
    "test_data_loader = DataLoader(test_finance_dataset, batch_size=24, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(train_data_loader)\n",
    "samples, labels = dataiter.next()\n",
    "# print(samples)\n",
    "# print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "input_size=samples.size(1) \n",
    "output_size=1\n",
    "hidden_dim=32\n",
    "n_layers=2\n",
    "\n",
    "rnn = RNN(input_size, output_size, hidden_dim, n_layers)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'rnn.pkl'\n",
    "\n",
    "def train(rnn, optimizer, epochs=1):\n",
    "\n",
    "    hidden = None      \n",
    "    min_loss = 100\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, data in enumerate(train_data_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "\n",
    "            prediction, hidden = rnn(inputs, hidden)\n",
    "            hidden = hidden.data\n",
    "\n",
    "            labels = labels.unsqueeze(1)\n",
    "            # calculate the loss\n",
    "            loss = criterion(prediction, labels)\n",
    "            # zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            # perform backprop and update weights\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if loss.item() < min_loss:\n",
    "                min_loss = loss.item()\n",
    "                save_checkpoint(rnn,optimizer,PATH)\n",
    "\n",
    "            # display loss and predictions\n",
    "            print('Epoch: {}, Loss: {} '.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(trained_rnn, optimizer, epochs=1):\n",
    "    hidden = None      \n",
    "    trained_rnn.eval()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_i, data in enumerate(test_data_loader):\n",
    "            inputs, labels = data\n",
    "\n",
    "            inputs = inputs.unsqueeze(0)\n",
    "            inputs = inputs.float()\n",
    "            labels = labels.float()\n",
    "            labels = labels.unsqueeze(1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                prediction, hidden = trained_rnn(inputs, hidden)\n",
    "                loss = criterion(prediction, labels)\n",
    "                print('Epoch: {}, Loss: {} '.format(epoch, loss.item()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 57.968231201171875 \n",
      "Epoch: 0, Loss: 57.85779571533203 \n",
      "Epoch: 0, Loss: 56.75642013549805 \n",
      "Epoch: 0, Loss: 53.58875274658203 \n",
      "Epoch: 0, Loss: 51.107975006103516 \n",
      "Epoch: 0, Loss: 48.3720703125 \n",
      "Epoch: 0, Loss: 43.92620849609375 \n",
      "Epoch: 0, Loss: 40.51518630981445 \n",
      "Epoch: 0, Loss: 37.05945587158203 \n",
      "Epoch: 0, Loss: 33.82000732421875 \n",
      "Epoch: 0, Loss: 25.600379943847656 \n",
      "Epoch: 0, Loss: 21.195905685424805 \n",
      "Epoch: 0, Loss: 19.47658348083496 \n",
      "Epoch: 0, Loss: 16.18085479736328 \n",
      "Epoch: 0, Loss: 14.630372047424316 \n",
      "Epoch: 0, Loss: 14.614950180053711 \n",
      "Epoch: 0, Loss: 13.1470365524292 \n",
      "Epoch: 0, Loss: 11.256665229797363 \n",
      "Epoch: 0, Loss: 9.32597827911377 \n",
      "Epoch: 0, Loss: 6.306405067443848 \n",
      "Epoch: 0, Loss: 3.430784225463867 \n",
      "Epoch: 0, Loss: 2.5012426376342773 \n",
      "Epoch: 0, Loss: 2.8264000415802 \n",
      "Epoch: 0, Loss: 2.3287596702575684 \n",
      "Epoch: 0, Loss: 1.5174857378005981 \n",
      "Epoch: 0, Loss: 1.4354310035705566 \n",
      "Epoch: 0, Loss: 0.48387405276298523 \n",
      "Epoch: 0, Loss: 0.3117234706878662 \n",
      "Epoch: 0, Loss: 0.0618428997695446 \n",
      "Epoch: 0, Loss: 0.13773559033870697 \n",
      "Epoch: 0, Loss: 0.31979697942733765 \n",
      "Epoch: 0, Loss: 0.1269499510526657 \n",
      "Epoch: 0, Loss: 0.040298689156770706 \n",
      "Epoch: 0, Loss: 0.03666764125227928 \n",
      "Epoch: 0, Loss: 0.008347704075276852 \n",
      "Epoch: 0, Loss: 0.0637160912156105 \n",
      "Epoch: 0, Loss: 0.05263174697756767 \n",
      "Epoch: 0, Loss: 0.04312194511294365 \n",
      "Epoch: 0, Loss: 0.012802124954760075 \n",
      "Epoch: 0, Loss: 0.028380732983350754 \n",
      "Epoch: 0, Loss: 0.04999397695064545 \n",
      "Epoch: 0, Loss: 0.019909031689167023 \n",
      "Epoch: 0, Loss: 0.047429412603378296 \n",
      "Epoch: 0, Loss: 0.3023607134819031 \n",
      "Epoch: 0, Loss: 0.5021677017211914 \n",
      "Epoch: 0, Loss: 1.4866632223129272 \n",
      "Epoch: 0, Loss: 1.4817835092544556 \n",
      "Epoch: 0, Loss: 2.010136127471924 \n",
      "Epoch: 0, Loss: 2.5459535121917725 \n",
      "Epoch: 0, Loss: 3.3438806533813477 \n",
      "Epoch: 0, Loss: 4.858981609344482 \n",
      "Epoch: 0, Loss: 6.747264862060547 \n",
      "Epoch: 0, Loss: 7.475499153137207 \n",
      "Epoch: 0, Loss: 7.448112964630127 \n",
      "Epoch: 0, Loss: 7.032858848571777 \n",
      "Epoch: 0, Loss: 6.626252174377441 \n",
      "Epoch: 0, Loss: 5.947240829467773 \n",
      "Epoch: 0, Loss: 5.137777805328369 \n",
      "Epoch: 0, Loss: 4.891029357910156 \n",
      "Epoch: 0, Loss: 4.197662830352783 \n",
      "Epoch: 0, Loss: 4.083940505981445 \n",
      "Epoch: 0, Loss: 4.149105072021484 \n",
      "Epoch: 0, Loss: 2.779752492904663 \n",
      "Epoch: 0, Loss: 2.1849451065063477 \n",
      "Epoch: 0, Loss: 2.2798383235931396 \n",
      "Epoch: 0, Loss: 2.2030160427093506 \n",
      "Epoch: 0, Loss: 2.316338062286377 \n",
      "Epoch: 0, Loss: 2.496309518814087 \n",
      "Epoch: 0, Loss: 2.4019339084625244 \n",
      "Epoch: 0, Loss: 2.249891757965088 \n",
      "Epoch: 0, Loss: 1.8698725700378418 \n",
      "Epoch: 0, Loss: 1.872011423110962 \n",
      "Epoch: 0, Loss: 1.5120022296905518 \n",
      "Epoch: 0, Loss: 1.688349962234497 \n",
      "Epoch: 0, Loss: 1.662684440612793 \n",
      "Epoch: 0, Loss: 1.9409807920455933 \n",
      "Epoch: 0, Loss: 1.7541673183441162 \n",
      "Epoch: 0, Loss: 1.2059829235076904 \n",
      "Epoch: 0, Loss: 1.4278372526168823 \n",
      "Epoch: 0, Loss: 1.5525071620941162 \n",
      "Epoch: 0, Loss: 1.0516256093978882 \n",
      "Epoch: 0, Loss: 1.2136194705963135 \n",
      "Epoch: 0, Loss: 1.3485515117645264 \n",
      "Epoch: 0, Loss: 1.2813361883163452 \n",
      "Epoch: 0, Loss: 1.1657676696777344 \n",
      "Epoch: 0, Loss: 0.971348226070404 \n",
      "Epoch: 0, Loss: 0.7861099243164062 \n",
      "Epoch: 0, Loss: 0.856451690196991 \n",
      "Epoch: 0, Loss: 0.7664065361022949 \n",
      "Epoch: 0, Loss: 0.6490454077720642 \n",
      "Epoch: 0, Loss: 0.37158137559890747 \n",
      "Epoch: 0, Loss: 0.07961225509643555 \n",
      "Epoch: 0, Loss: 0.041124071925878525 \n",
      "Epoch: 0, Loss: 0.026868365705013275 \n",
      "Epoch: 0, Loss: 0.016484985128045082 \n",
      "Epoch: 0, Loss: 0.05695841833949089 \n",
      "Epoch: 0, Loss: 0.0061525688506662846 \n",
      "Epoch: 0, Loss: 0.04385789483785629 \n",
      "Epoch: 0, Loss: 0.09649579226970673 \n",
      "Epoch: 0, Loss: 0.08372480422258377 \n",
      "Epoch: 0, Loss: 0.2140706479549408 \n",
      "Epoch: 0, Loss: 0.15055640041828156 \n",
      "Epoch: 0, Loss: 0.4959656894207001 \n",
      "Epoch: 0, Loss: 0.7685152292251587 \n",
      "Epoch: 0, Loss: nan \n",
      "Epoch: 0, Loss: 1.4230420589447021 \n",
      "Epoch: 0, Loss: 1.889178991317749 \n",
      "Epoch: 0, Loss: 2.170616865158081 \n",
      "Epoch: 0, Loss: 2.4403722286224365 \n",
      "Epoch: 0, Loss: 2.169771909713745 \n",
      "Epoch: 0, Loss: 2.8217697143554688 \n",
      "Epoch: 0, Loss: 3.5266904830932617 \n",
      "Epoch: 0, Loss: 3.2006099224090576 \n",
      "Epoch: 0, Loss: 3.739118814468384 \n",
      "Epoch: 0, Loss: 3.4002931118011475 \n",
      "Epoch: 0, Loss: nan \n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "\n",
    "train(rnn, optimizer)\n",
    "trained_rnn, optimizer = load_checkpoint(rnn,optimizer,PATH)\n",
    "test(trained_rnn, optimizer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
